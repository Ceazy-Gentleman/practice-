{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TimeSeries.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPP6byTig41eaJaD5a+HXGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ceazy-Gentleman/practice-/blob/master/TimeSeriesPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdxod7bekLE"
      },
      "source": [
        "# 安装并载入Gluonts工具包\n",
        "示例中gluonts.trainer 变为了 gluonts.mx.trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLT13Jn7fHf9"
      },
      "source": [
        "pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6lnaisHmfKSV"
      },
      "source": [
        "pip install mxnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7ozAz8PTntHJ"
      },
      "source": [
        "pip install gluonts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIdJ7X76oD6S"
      },
      "source": [
        "from gluonts.model.deepar import DeepAREstimator\n",
        "from gluonts.mx.trainer import Trainer\n",
        "estimator = DeepAREstimator(freq=\"5min\",prediction_length=36,trainer=Trainer(epochs=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR6tBvz2uSiR"
      },
      "source": [
        "# 内置的数据集\n",
        "In general, the datasets provided by GluonTS are objects that consists of three main members:\n",
        "\n",
        "- dataset.train is an iterable collection of data entries used for training. Each entry corresponds to one time series\n",
        " \n",
        "- dataset.test is an iterable collection of data entries used for inference. The test dataset is an extended version of the train dataset that contains a window in the end of each time series that was not seen during training. This window has length equal to the recommended prediction length.\n",
        "\n",
        "- dataset.metadata contains metadata of the dataset such as the frequency of the time series, a recommended prediction horizon, associated features, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qFKe8MVuY51"
      },
      "source": [
        "from gluonts.dataset.repository.datasets import get_dataset, dataset_recipes\n",
        "from gluonts.dataset.util import to_pandas\n",
        "print(f\"Available datasets: {list(dataset_recipes.keys())}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIBYTUdauzfI"
      },
      "source": [
        "dataset = get_dataset(\"m4_hourly\", regenerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuGLS7TdyAV4"
      },
      "source": [
        "entry = next(iter(dataset.train))\n",
        "train_series = to_pandas(entry)\n",
        "train_series.plot()\n",
        "plt.grid(which=\"both\")\n",
        "plt.legend([\"train series\"], loc=\"upper left\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpRzeWZ3yNyV"
      },
      "source": [
        "entry = next(iter(dataset.test))\n",
        "test_series = to_pandas(entry)\n",
        "test_series.plot()\n",
        "plt.axvline(train_series.index[-1], color='r') # end of train dataset\n",
        "plt.grid(which=\"both\")\n",
        "plt.legend([\"test series\", \"end of train series\"], loc=\"upper left\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou3ap9hU0GEj"
      },
      "source": [
        "print(f\"Length of forecasting window in test dataset: {len(test_series) - len(train_series)}\")\n",
        "print(f\"Recommended prediction horizon: {dataset.metadata.prediction_length}\")\n",
        "print(f\"Frequency of the time series: {dataset.metadata.freq}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl02vanP0a5x"
      },
      "source": [
        "# 自定义数据集\n",
        "At this point, it is important to emphasize that GluonTS does not require this specific format for a custom dataset that a user may have. The only requirements for a custom dataset are to be iterable and have a “target” and a “start” field. To make this more clear, assume the common case where a dataset is in the form of a numpy.array and the index of the time series in a pandas.Timestamp (possibly different for each time series):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmhxyBPV0kT0"
      },
      "source": [
        "N = 10  # number of time series\n",
        "T = 100  # number of timesteps\n",
        "prediction_length = 24\n",
        "freq = \"1H\"\n",
        "custom_dataset = np.random.normal(size=(N, T))\n",
        "start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj3TnUsX1eC0"
      },
      "source": [
        "Now, you can split your dataset and bring it in a GluonTS appropriate format with just two lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdSxIHXm1rFY"
      },
      "source": [
        "from gluonts.dataset.common import ListDataset\n",
        "# train dataset: cut the last window of length \"prediction_length\", add \"target\" and \"start\" fields\n",
        "train_ds = ListDataset(\n",
        "    [{'target': x, 'start': start} for x in custom_dataset[:, :-prediction_length]],\n",
        "    freq=freq\n",
        ")\n",
        "# test dataset: use the whole dataset, add \"target\" and \"start\" fields\n",
        "test_ds = ListDataset(\n",
        "    [{'target': x, 'start': start} for x in custom_dataset],\n",
        "    freq=freq\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsh6fCqe31l6"
      },
      "source": [
        "# 模型调用方法\n",
        "GluonTS comes with a number of pre-built models. All the user needs to do is configure some hyperparameters. The existing models focus on (but are not limited to) probabilistic forecasting. Probabilistic forecasts are predictions in the form of a probability distribution, rather than simply a single point estimate.\n",
        "\n",
        "We will begin with GulonTS’s pre-built feedforward neural network estimator, a simple but powerful forecasting model. We will use this model to demonstrate the process of training a model, producing forecasts, and evaluating the results.\n",
        "\n",
        "GluonTS’s built-in feedforward neural network (SimpleFeedForwardEstimator) accepts an input window of length context_length and predicts the distribution of the values of the subsequent prediction_length values. In GluonTS parlance, the feedforward neural network model is an example of Estimator. In GluonTS, Estimator objects represent a forecasting model as well as details such as its coefficients, weights, etc.\n",
        "\n",
        "In general, each estimator (pre-built or custom) is configured by a number of hyperparameters that can be either common (but not binding) among all estimators (e.g., the prediction_length) or specific for the particular estimator (e.g., number of layers for a neural network or the stride in a CNN).\n",
        "\n",
        "Finally, each estimator is configured by a Trainer, which defines how the model will be trained i.e., the number of epochs, the learning rate, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Px9LKSj4HLm"
      },
      "source": [
        "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
        "from gluonts.mx import Trainer\n",
        "estimator = SimpleFeedForwardEstimator(\n",
        "    num_hidden_dimensions=[10],\n",
        "    prediction_length=dataset.metadata.prediction_length,\n",
        "    context_length=100,\n",
        "    freq=dataset.metadata.freq,\n",
        "    trainer=Trainer(\n",
        "        ctx=\"cpu\",\n",
        "        epochs=5,\n",
        "        learning_rate=1e-3,\n",
        "        num_batches_per_epoch=100\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmoIWe2S4iNH"
      },
      "source": [
        "After specifying our estimator with all the necessary hyperparameters we can train it using our training dataset dataset.train by invoking the train method of the estimator. The training algorithm returns a fitted model (or a Predictor in GluonTS parlance) that can be used to construct forecasts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Hv88UV4kxv"
      },
      "source": [
        "predictor = estimator.train(dataset.train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S-aEDdX42Jx"
      },
      "source": [
        "# 可视化及评估预测结果\n",
        "With a predictor in hand, we can now predict the last window of the dataset.test and evaluate our model’s performance.\n",
        "\n",
        "GluonTS comes with the make_evaluation_predictions function that automates the process of prediction and model evaluation. Roughly, this function performs the following steps:\n",
        "\n",
        "- Removes the final window of length prediction_length of the dataset.test that we want to predict\n",
        "\n",
        "- The estimator uses the remaining data to predict (in the form of sample paths) the “future” window that was just removed\n",
        "\n",
        "- The module outputs the forecast sample paths and the dataset.test (as python generator objects)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIn9JDoC6iEf"
      },
      "source": [
        "from gluonts.evaluation import make_evaluation_predictions\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=dataset.test,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBxmub27hSV"
      },
      "source": [
        "First, we can convert these generators to lists to ease the subsequent computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAh7GLq17s1_"
      },
      "source": [
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyXL7LY-8EmG"
      },
      "source": [
        "We can examine the first element of these lists (that corresponds to the first time series of the dataset). Let’s start with the list containing the time series, i.e., tss. We expect the first entry of tss to contain the (target of the) first time series of dataset.test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFLdB12u8Gri"
      },
      "source": [
        "# first entry of the time series list\n",
        "ts_entry = tss[0]\n",
        "# first 5 values of the time series (convert from pandas to numpy)\n",
        "print(np.array(ts_entry[:5]).reshape(-1,))\n",
        "# first entry of dataset.test\n",
        "dataset_test_entry = next(iter(dataset.test))\n",
        "# first 5 values\n",
        "dataset_test_entry['target'][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM_yzNhQ9oHr"
      },
      "source": [
        "The entries in the forecast list are a bit more complex. They are objects that contain all the sample paths in the form of numpy.ndarray with dimension (num_samples, prediction_length), the start date of the forecast, the frequency of the time series, etc. We can access all these information by simply invoking the corresponding attribute of the forecast object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6wLTbju9qMj"
      },
      "source": [
        "\n",
        "# first entry of the forecast list\n",
        "forecast_entry = forecasts[0]\n",
        "print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
        "print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
        "print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
        "print(f\"Frequency of the time series: {forecast_entry.freq}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR5EZH6Y-cI9"
      },
      "source": [
        "We can also do calculations to summarize the sample paths, such computing the mean or a quantile for each of the 48 time steps in the forecast window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MK_Lv3qn-ear"
      },
      "source": [
        "print(f\"Mean of the future window:\\n {forecast_entry.mean}\")\n",
        "print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WlspCXj-6QS"
      },
      "source": [
        "Forecast objects have a plot method that can summarize the forecast paths as the mean, prediction intervals, etc. The prediction intervals are shaded in different colors as a “fan chart”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUEpYgfG_Ll0"
      },
      "source": [
        "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
        "    plot_length = 150\n",
        "    prediction_intervals = (50.0, 90.0)\n",
        "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
        "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
        "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
        "    plt.grid(which=\"both\")\n",
        "    plt.legend(legend, loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3T1okNo_TnS"
      },
      "source": [
        "plot_prob_forecasts(ts_entry, forecast_entry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCu57k_lBCv3"
      },
      "source": [
        "We can also evaluate the quality of our forecasts numerically. In GluonTS, the Evaluator class can compute aggregate performance metrics, as well as metrics per time series (which can be useful for analyzing performance across heterogeneous time series).\n",
        "\n",
        "我们也可以用数字来评估我们的预测质量。在GluonTS中，评估器类可以计算总的性能指标，以及每个时间序列的指标（这对于分析不同时间序列的性能非常有用）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5i6EXSGBqL_"
      },
      "source": [
        "from gluonts.evaluation import Evaluator\n",
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(dataset.test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MvLfiR8CBB-"
      },
      "source": [
        "- Aggregate metrics are aggregated both across time-steps and across time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La5JzQS_CM3G"
      },
      "source": [
        "import json\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbkj0osHCXUL"
      },
      "source": [
        "- Individual metrics are aggregated only across time-steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWn1pODsCqWU"
      },
      "source": [
        "item_metrics.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33fQxQeFC-3M"
      },
      "source": [
        "item_metrics.plot(x='MSIS', y='MASE', kind='scatter')\n",
        "plt.grid(which=\"both\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlOVhUyOoXmx"
      },
      "source": [
        "# 时序预测案例\n",
        "## 导入数据集\n",
        "  使用一个记录了提到 AMZN 股票代号的推特数量的公开数据集，用 pandas 下载并显示这个数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPpsbW4OqDdQ"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv\"\n",
        "df = pd.read_csv(url, header=0, index_col=0)\n",
        "\n",
        "df[:200].plot(figsize=(20, 5), linewidth=2)\n",
        "plt.grid()\n",
        "plt.legend([\"observations\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8OxnPQFqJlX"
      },
      "source": [
        "## 划分数据集\n",
        "GluonTS 提供了 Dataset 抽象层来统一各种不同输入格式的读取。在这里，我们使用 ListDataset 来读取在内存中以字典列表形式存储的数据。在 GluonTS 中，任何 Dataset 对象都是一个将字符串键值映射到任意值的字典的迭代器。 将数据截断到 2015 年 4 月 5 日为止，用于训练模型。在 4 月 5 日之后的数据将被用于测试训练好的模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SrHOe-pqdb1"
      },
      "source": [
        "from gluonts.dataset.common import ListDataset\n",
        "training_data = ListDataset(\n",
        "    [{\"start\": df.index[0], \"target\": df.value[:\"2015-04-05 00:00:00\"]}],\n",
        "    freq = \"5min\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPHTwcp6rBtt"
      },
      "source": [
        "## 导入训练集进行训练\n",
        "数据集在手，现在就可以使用刚才建立的 Estimator 的 train 接口来训练模型。当训练完成之后，你就会得到一个可以进行预测的 Predictor 对象。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf00wyY9rPAP"
      },
      "source": [
        "predictor = estimator.train(training_data=training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNamhPhkrTxL"
      },
      "source": [
        "## 评估模型\n",
        "现在你可以使用 Predictor 来画出模型对于在训练数据之后的时间段的一些预测。绘出模型给出的预测有助于我们对这个模型的预测质量有一个定性的感受。 现在，基于同样的数据集，在之前用于训练的时间段之后的时间段取出若干组测试数据。如下图所示，模型给出的是概率预测，这是很重要的一点，因为概率预测提供了对于模型置信度的估计，并且可以使得下游基于此预测的决策能够考虑到预测的不确定性。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnjuPaVsVMU"
      },
      "source": [
        "test_data = ListDataset(\n",
        "    [\n",
        "        {\"start\": df.index[0], \"target\": df.value[:\"2015-04-10 03:00:00\"]},\n",
        "        {\"start\": df.index[0], \"target\": df.value[:\"2015-04-15 18:00:00\"]},\n",
        "        {\"start\": df.index[0], \"target\": df.value[:\"2015-04-20 12:00:00\"]}\n",
        "    ],\n",
        "    freq = \"5min\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbQC2nIcsfXw"
      },
      "source": [
        "from itertools import islice\n",
        "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
        "\n",
        "def plot_forecasts(tss, forecasts, past_length, num_plots):\n",
        "    for target, forecast in islice(zip(tss, forecasts), num_plots):\n",
        "        ax = target[-past_length:].plot(figsize=(12, 5), linewidth=2)\n",
        "        forecast.plot(color='g')\n",
        "        plt.grid(which='both')\n",
        "        plt.legend([\"observations\", \"median prediction\", \"90% confidence interval\", \"50% confidence interval\"])\n",
        "        plt.show()\n",
        "\n",
        "forecast_it, ts_it = make_evaluation_predictions(test_data, predictor=predictor, num_samples=100)\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "plot_forecasts(tss, forecasts, past_length=150, num_plots=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ITNlhyZsonD"
      },
      "source": [
        "看起来预测还算准确！现在我们可以定量地使用一系列指标来定量评估预测的质量。GluonTS 提供了用于评估模型的 Evaluator 模块。Evaluator 模块提供了常用的误差指标，例如 MSE、MASE、symmetric MAPE、RMSE 以及（加权）量化误差等。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNcL3tVtdTj"
      },
      "source": [
        "from gluonts.evaluation import Evaluator\n",
        "evaluator = Evaluator(quantiles=[0.5], seasonality=2016)\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_data))\n",
        "agg_metrics\n",
        "{'MSE': 163.59102376302084,\n",
        " 'abs_error': 1090.9220886230469,\n",
        " 'abs_target_sum': 5658.0,\n",
        " 'abs_target_mean': 52.38888888888889,\n",
        " 'seasonal_error': 18.833625618877182,\n",
        " 'MASE': 0.5361500323952336,\n",
        " 'sMAPE': 0.21201368270827592,\n",
        " 'MSIS': 21.446000940010823,\n",
        " 'QuantileLoss[0.5]': 1090.9221000671387,\n",
        " 'Coverage[0.5]': 0.34259259259259256,\n",
        " 'RMSE': 12.790270668090681,\n",
        " 'NRMSE': 0.24414090352665138,\n",
        " 'ND': 0.19281054942082837,\n",
        " 'wQuantileLoss[0.5]': 0.19281055144346743,\n",
        " 'mean_wQuantileLoss': 0.19281055144346743,\n",
        " 'MAE_Coverage': 0.15740740740740744}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkUDxMeQtfoK"
      },
      "source": [
        "## 方法比较\n",
        "你可以将以上指标与其他模型或是你的预测应用的业务要求作比较。例如，我们可以用 Seasonal Naive Method 进行预测，然后与以上结果比较。Seasonal Naive Method 假设数据会有一个固定的周期性（本例中，2016 个数据点是一周，作为一个周期），并通过基于周期性来复制之前观察到的训练数据进行预测。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9IaMeqKt4vl"
      },
      "source": [
        "from gluonts.model.seasonal_naive import SeasonalNaivePredictor\n",
        "\n",
        "seasonal_predictor_1W = SeasonalNaivePredictor(freq=\"5min\", prediction_length=36, season_length=2016)\n",
        "\n",
        "forecast_it, ts_it = make_evaluation_predictions(test_data, predictor=seasonal_predictor_1W, num_samples=100)\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "agg_metrics_seasonal, item_metrics_seasonal = evaluator(iter(tss), iter(forecasts), num_series=len(test_data))\n",
        "\n",
        "df_metrics = pd.DataFrame.join(\n",
        "    pd.DataFrame.from_dict(agg_metrics, orient='index').rename(columns={0: \"DeepAR\"}),\n",
        "    pd.DataFrame.from_dict(agg_metrics_seasonal, orient='index').rename(columns={0: \"Seasonal naive\"})\n",
        ")\n",
        "\n",
        "df_metrics.loc[[\"MASE\", \"sMAPE\", \"RMSE\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XamasDGot6vM"
      },
      "source": [
        "通过比较以上这些指标，就可以得到你的模型与基线模型或其他高阶模型的对比。想要进一步提升模型性能的话，可以尝试改进模型架构，或者调节超参数的值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdJylCYQuFZ7"
      },
      "source": [
        "# 时序异常检测方法案例\n",
        "本例中使用DeepAR的方法对电力系统中的异常案例进行检测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It7AClhTumyt"
      },
      "source": [
        "import numpy as np\n",
        "from itertools import islice\n",
        "from functools import partial\n",
        "import mxnet as mx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas.plotting import register_matplotlib_converters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp50xvu0vAgO"
      },
      "source": [
        "register_matplotlib_converters()\n",
        "\n",
        "from gluonts.dataset.loader import TrainDataLoader\n",
        "from gluonts.model.deepar import DeepAREstimator\n",
        "from gluonts.mx.util import get_hybrid_forward_input_names\n",
        "from gluonts.mx.trainer import Trainer\n",
        "from gluonts.mx.batchify import batchify\n",
        "from gluonts.dataset.repository.datasets import get_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6fQf5-vvHsF"
      },
      "source": [
        "dataset = get_dataset(dataset_name=\"electricity\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oPl8JWKvLQx"
      },
      "source": [
        "estimator = DeepAREstimator(\n",
        "        prediction_length=dataset.metadata.prediction_length,\n",
        "        freq=dataset.metadata.freq,\n",
        "        trainer=Trainer(\n",
        "            learning_rate=1e-3, epochs=20, num_batches_per_epoch=100\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BbpeOfbwgE2"
      },
      "source": [
        "train_output = estimator.train_model(dataset.train)\n",
        "input_names = get_hybrid_forward_input_names(\n",
        "        type(train_output.trained_net)\n",
        "    )\n",
        "batch_size = 500\n",
        "num_samples = 100\n",
        "instance_splitter = estimator._create_instance_splitter(\"training\")\n",
        "training_data_loader = TrainDataLoader(\n",
        "    dataset=dataset.train,\n",
        "     transform=train_output.transformation + instance_splitter,\n",
        "    batch_size=batch_size,\n",
        "    num_batches_per_epoch=estimator.trainer.num_batches_per_epoch,\n",
        "    stack_fn=partial(\n",
        "        batchify, ctx=estimator.trainer.ctx, dtype=estimator.dtype\n",
        "    ),\n",
        ")\n",
        "\n",
        "data_entry = next(iter(training_data_loader))\n",
        "\n",
        "# we now call the train model to get the predicted distribution on each window\n",
        "# this allows us to investigate where are the biggest anomalies\n",
        "context_length = train_output.trained_net.context_length\n",
        "prediction_length = train_output.trained_net.prediction_length\n",
        "\n",
        "distr = train_output.trained_net.distribution(\n",
        "    *[data_entry[k] for k in input_names]\n",
        ")\n",
        "\n",
        "# gets all information into numpy array for further plotting\n",
        "samples = distr.sample(num_samples).asnumpy()\n",
        "percentiles = np.percentile(samples, axis=0, q=[10.0, 90.0])\n",
        "target = mx.ndarray.concat(data_entry[\"past_target\"], data_entry[\"future_target\"], dim=1\n",
        ")\n",
        "target = target[:, -(context_length + prediction_length) :]\n",
        "nll = -distr.log_prob(target).asnumpy()\n",
        "target = target.asnumpy()\n",
        "mean = samples.mean(axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30jQdsJJxdKm"
      },
      "source": [
        "# NLL indices from largest to smallest\n",
        "sorted_indices = np.argsort(nll.reshape(1,-1))[::-1]\n",
        "\n",
        "# shows the series and times when the 20 largest NLL were observed\n",
        "for k in sorted_indices[:20].reshape(-1):\n",
        "    i = k // nll.shape[1]\n",
        "    t = k % nll.shape[1]\n",
        "\n",
        "    time_index = pd.date_range(start = pd.Timestamp(data_entry[\"forecast_start\"][i]),periods = context_length + prediction_length)\n",
        "        \n",
        "    time_index -= context_length * time_index.freq\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.fill_between(\n",
        "        time_index,\n",
        "        percentiles[0, i],\n",
        "        percentiles[-1, i],\n",
        "        alpha=0.5,\n",
        "        label=\"80% CI predicted\"\n",
        "    )\n",
        "    plt.plot(time_index, target[i], label=\"target\")\n",
        "    plt.axvline(time_index[t], alpha=0.5, color=\"r\")\n",
        "    plt.title(f\"NLL: {nll[i, t]}\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dYToTqU5ixa"
      },
      "source": [
        "type(data_entry[\"forecast_start\"][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMdPmnN6f5V"
      },
      "source": [
        "prediction_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5sLgJkr7Z6T"
      },
      "source": [
        "nll.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCAL-Qe-MuLL"
      },
      "source": [
        "nll.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yef2ZlhsND-W"
      },
      "source": [
        "time_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrO39cQ3ZGIR"
      },
      "source": [
        "percentiles.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYr5hWoJc4as"
      },
      "source": [
        "sorted_indices[:20].reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXWrheeadzrJ"
      },
      "source": [
        "6316//48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsZIy5J-d_ZL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}